{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b28ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoModelForMaskedLM, AutoModelForCausalLM\n",
    "import torch, datasets, sacremoses\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"flax-community/papuGaPT2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf0698",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1efdcbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples, block_size=512):\n",
    "    \n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = (len(concatenated[\"input_ids\"]) // block_size) * block_size\n",
    "\n",
    "    result = {}\n",
    "    for k, v in concatenated.items():\n",
    "        result[k] = [v[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe6a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"text\", data_files={\n",
    "   \"train\": \"pan_tadeusz_1_10.txt\",\n",
    "   \"validation\": \"pan_tadeusz_11.txt\",\n",
    "   \"test\": \"pan_tadeusz_12.txt\",\n",
    "})\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "   return tokenizer(examples[\"text\"])\n",
    "\n",
    "\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65791225-d752-41e3-a3ab-3775a038a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    logits = torch.tensor(logits)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    shift_logits = logits[..., :-1, :].reshape(-1, logits.shape[-1])\n",
    "    shift_labels = labels[..., 1:].reshape(-1)\n",
    "    loss = F.cross_entropy(shift_logits, shift_labels)\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return {\"perplexity\": perplexity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b911f38-2aa6-4060-8523-7f5c5411f509",
   "metadata": {},
   "source": [
    "## Base check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d829ef-8a83-4aff-aaf7-722f397dc866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text before training:\n",
      " Jam jest Jacek, ale ten jest po to, żeby służyć w tym świętym miejscu. On jest dla ciebie i dla ciebie. Ja jestem, a ty jesteś z tego, a więc twoja dusza jest ze mną, czy to jesteś Ty, czy to jest twoja dusza i twoja dusza. Ja jestem twoim Ojcem, i ty jesteś ze mną, a więc twoja dusza i twoja dusza jest ze mną. Ja jestem twoim Ojcem i ty jesteś ze mną. Ja jestem twój Ojciec, i ty jesteś moim Ojcem. To jest to, co robię, w jaki sposób robię, to, co mówię, to, co robię. Ja jestem twoim Ojcem i ty jesteś moim Ojcem, to, co robię, co mówię, to, co robię, to, co mówię. Ja jestem twoim Ojcem, a ty jesteś moim Ojcem, a więc twój Ojciec jest twoim Ojcem, ja jestem twoim Ojcem. Ja jestem twoim Ojcem i ty jesteś moim Ojcem. To jest to, co\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "prompt = \"Jam jest Jacek\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=200,\n",
    "        do_sample=True,\n",
    "        temperature=.9,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text before training:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2250b84",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b2a57e-b63d-4f31-ab30-617d26012c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class GenerateTextCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, prompt=\"Jam jest Jacek\", max_length=200):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt = prompt\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs['model']\n",
    "        model.eval()\n",
    "\n",
    "        input_ids = self.tokenizer(self.prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_length=self.max_length,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                temperature=.9,\n",
    "                num_return_sequences=1\n",
    "            ) \n",
    "        \n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"\\n--- Generated text after epoch {state.epoch}:\")\n",
    "        print(generated_text)\n",
    "        print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a5eceb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\litwi\\AppData\\Local\\Temp\\ipykernel_31952\\1012462473.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 07:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.834122</td>\n",
       "      <td>125.726967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.761708</td>\n",
       "      <td>116.944138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.696200</td>\n",
       "      <td>4.731475</td>\n",
       "      <td>113.461487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.696200</td>\n",
       "      <td>4.717376</td>\n",
       "      <td>111.873055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.408800</td>\n",
       "      <td>4.713063</td>\n",
       "      <td>111.391479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated text after epoch 1.0:\n",
      "Jam jest Jacek, że go tu przyprowadziłeś i za kim on stał, że za nim z Tobą stał, że za nim wołał i wołał? Bo ja jestem Jacek, ja mu, Jacku, mówię, że cię szukam, ale on nie może mnie znaleźć; ja go nie znajdę; on szuka mnie; ja nie znajdę, ja go nie znajdę; ja go nie znajdę; ja nie znajdę, ja go nie znajdę; ja go nie znajdę; ja go nie znajdę; ja go nie znajdę, ja go nie znajdę; ja go nie znajdę, ja go nie znajdę; ja go nie znajdę; ja go nie znajdę, ja go nie znajdę; ja go nie znajdę; ja go nie znajdę; ja go nie znajdę; ja go nie znajdę, ja go nie znajdę; ja go nie znajdę, ja go nie znajdę; ja go nie znajdę, ja go nie znajdę; ja go nie znajdę, ja go nie znajdę; ja go nie znajdę; ja go nie znajdę, ja\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated text after epoch 2.0:\n",
      "Jam jest Jacek! W imieniu Pana Starosty - to wszystko prawda. A my razem z nim.\n",
      "Pan Starosta, co to, w końcu z Panem Starostą, razem, z nim? Bo jak tu postąpić z panem, co? Pan, co Pan tu pisze? Nie, pan już, panie Starosto, za panem, tak w końcu idzie. Nie, ja tak nie mogę, Panie, ja tu przecież jestem, nie powiem, ale powiem: tu, nie wiadomo czemu, nie można. W tym celu, by tak się mogło stać, że pan się tu znajduje, w tym miejscu, i to nie w tym, czy tam, tylko tam. A nie tam, panie starosto, nie tam, proszę pana, ja jestem! Tak, pan jest tu w tym miejscu, a ja tu, w tym miejscu, w tym miejscu!\n",
      "A ja tu jestem, mówię: jestem, bo ja jestem, bo ja jestem, ja jestem, ja jestem, ja jestem! Pan\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated text after epoch 3.0:\n",
      "Jam jest Jacek, jak wróbel, bo jak go widzisz, to się zwiążesz. Tak Bóg umiłował człowieka, że Syna swego przeznaczył, aby był kuszony, lecz w tym celu, aby był kuszony, a świat go kuszony. Niechaj więc Bóg was miłuje i strzeże, i mnie strzeże, abym nigdy was nie obraził; abyście w nienawiści nie byli obcymi; albowiem Pan Bóg, jak zapowiedział, w niebie wsławił nas i nas umiłował.\n",
      "Rzekł do Niego Jezus: Panie, a co tu robisz?; Panie, a gdzie idziesz?; Panie, a gdzie się udasz?; Panie, a gdzie pościsz?;Panie, a gdzie mieszkasz?; Panie, a gdzie mieszkasz?;Panie, a gdzie pościsz?;Panie, a gdzie jesteś?;Panie, a gdzie pość chcesz?;Panie, a gdzie jesteś?;Panie, a gdzie pość jesteś?;\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated text after epoch 4.0:\n",
      "Jam jest Jacek, niechże się Jacek zająknie i jak człowiek z daleka, niech spojrzy na niego; lecz kto go nie zobaczy, ten nie będzie jaśniał z jego oczu, jako jaśniejący Janek z dalekich krajów».    «Jeruzalem! niech jaśnieje w całym twoim świecie!». «Panie Boże! Panie Boże! Boże, jaśniejesz! niech jaśniejesz! niech jaśniejesz!» «Och, niechaj jaśniejesz!» «Panie Boże! niechaj jaśniejesz!» «Oj, niechaj jaśniejesz!» «O! niechaj jaśniejesz!» «Oj, niechaj jaśniejesz!». «Niechaj jaśniejesz!» «Oj, niechaj jaśniejesz!» «Panie Boże! jaśniejesz!». «Niechaj Jaśniejesz!» «Panie Boże! Boże, jaśniejesz!» «Panie Boże, jaśniejesz!» «Nie\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated text after epoch 5.0:\n",
      "Jam jest Jacek, to znaczy ja, to znaczy Jacek, jak ja mówię: «Ja, Jacek!» «Ja, Jacek!» «Ja, Jacek!» «Ja, Jacek!» «Ja, Jacek!» «Ja, Jacek!» «Ja, Jacek!» «Ja, Jacek!» «Ja, Jacek!» «Ja, Jacek!» «Ja, Jacek!». Jacek ma trzynaście lat i w tym wieku jest już człowiekiem dojrzałym, bardzo gotowym do służby Bożej.Wziął, co mu się podobało, wjechał do miasta, gdzie go w domu zastali.Wziął do roboty. W tej robocie widział, że w mieście nie było dla niego miejsca.W mieście nie było miejsca, gdzie byłby panować nad tymi ludźmi.To są ludzie młodzi, a młodzież w mieście jest bardzo duża.Jak pan widzi, pan był we wsi bardzo dobrze zżyty i w tej wsi, więc pan ma się stać na miejscu moim\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=4.516413879394531, metrics={'train_runtime': 447.6973, 'train_samples_per_second': 2.122, 'train_steps_per_second': 0.268, 'total_flos': 248227430400000.0, 'train_loss': 4.516413879394531, 'epoch': 5.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./decoder-pan-tadeusz\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"perplexity\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[GenerateTextCallback(tokenizer),  EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
